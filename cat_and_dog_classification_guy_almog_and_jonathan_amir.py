# -*- coding: utf-8 -*-
"""Cat and Dog Classification - Guy Almog and Jonathan Amir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fp2V_B1kYycnXxLILftqlQ6n-ekCJURB

#Load Dataset from Kaggle
https://www.kaggle.com/datasets/zippyz/cats-and-dogs-breeds-classification-oxford-dataset

Note: For presentation purposes, a personal Kaggle account is used to import the dataset on-the-fly.
"""

import json
creds = {"username":"djohoe28","key":"0d2177ae324b2b0c10bd24be26503ce8"}
json_object = json.dumps(creds, indent=4)
with open("kaggle.json", "w") as outfile:
    outfile.write(json_object)
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!pip install kaggle
!kaggle datasets download -d zippyz/cats-and-dogs-breeds-classification-oxford-dataset
!unzip cats-and-dogs-breeds-classification-oxford-dataset

"""#Initialize Dataset"""

import os

ROOT = ''
IMGS_PATH = 'images/images'

# excluded images that do not work correctly
bad = {'Abyssinian_34.jpg', 'Egyptian_Mau_145.jpg', 'Egyptian_Mau_139.jpg', 'Egyptian_Mau_191.jpg', 'Egyptian_Mau_177.jpg', 'Egyptian_Mau_167.jpg'}

all_imgs = [i for i in os.listdir(IMGS_PATH) if i.rsplit('.',1)[1] == 'jpg' and i not in bad]
print(str(len(all_imgs)), " images loaded successfully.")

"""#Read Dataset"""

l = open(ROOT + 'annotations/annotations/list.txt', 'r')

# lambda functions to get breed and species from the annotations
get_breed = lambda pic : pic.rsplit('_',1)[0].lower()
get_species = lambda num : 'cat' if num==1 else 'dog'

info_by_id = {}
info_by_breed = {}

# populate dictionary from list.txt
for line in l:
  # ignore commented lines
  if line[0] == '#':
    continue
  
  line = line.strip().split(' ')
  species = get_species(int(line[2]))
  id = int(line[1])
  breedid = int(line[3])
  name = get_breed(line[0]).lower()
  
  if name not in info_by_breed:
    info_by_breed[name] = {'breed' : name, 'species' : species, 'globalid': id, 'breedid':breedid}
    info_by_id[id] = info_by_breed[name]

"""#Create Model"""

!pip install keras

# more imports
import tensorflow as tf
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.optimizers import Adam

def getModel(dropout=.25, learning_rate=0.001, augmentation=False):
  # 'frozen' base model
  base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))
  base_model.trainable = False

  # input layer
  inputs = tf.keras.Input(shape=(299, 299, 3))
  x = inputs
  
  # pass results to a preprocessing layer
  x = tf.keras.applications.inception_v3.preprocess_input(x)
  
  # send results to the already trained model
  x = base_model(x, training=False)
  
  # send results to pooling layer
  x = tf.keras.layers.GlobalAveragePooling2D()(x)
  
  # send to fully connected layer
  x = tf.keras.layers.Dense(256,activation='relu')(x)
  
  # send to dropout layer
  x = tf.keras.layers.Dropout(dropout)(x)
  
  # send to batch normalization layer
  x = tf.keras.layers.BatchNormalization()(x)
   
  # send to output dense layer with SoftMax activation function
  outputs = tf.keras.layers.Dense(37,activation='softmax')(x)
  
  # create a model with these layers
  model = tf.keras.Model(inputs, outputs)
  
  # show some information
  model.summary()

  # compile model with Adam optimizer
  # and chosing accuracy as the metric to evaluate the model performance during training
  model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])

  return model

"""#Prepare Images (Batch Normalization)"""

# some more imports
import random
import cv2
import numpy as np

# setting image pixel side size
IMG_SIZE = 299

def getXy():
  # function that returns the number correspondent to the breed of 
  # the animal in the image, given the image name
  get_class_no = lambda name : info_by_breed[get_breed(name)]['globalid']
  
  # all image tensors will be stored here after resizing
  training_data = []
  
  for img in all_imgs:
    path = os.path.join(IMGS_PATH, img)
  
    # this is a trick to make the image be opened in RGB format, which is not the default
    img_array = cv2.imread(path)[...,::-1] 

    # here the images are rezise
    img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
  
    # get the ID of the image class
    class_no = get_class_no(img)
  
    training_data.append([img_array, class_no])
    
  # data should be in random order to improve performance
  random.shuffle(training_data)
  
  # separating data from list
  training = list(zip(*training_data))
  X = training[0]
  y = training[1]
  
  # transforming X to an np.array and resizing
  X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
  y = np.array(y)
  return X, y

"""#Split to Training/Testing"""

from sklearn.model_selection import train_test_split

X, y = getXy()
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

"""Note: Here we limit the dataset to 2000 training & 500 testing units for a reasonable runtime."""

x_train_short = x_train[:2000,:,:,:]
x_test_short = x_test[:500,:,:,:]
y_train_short = y_train[:2000]
y_test_short = y_test[:500]

"""#Data Preparation
Encode the properties of the data into numeric form, then prepare for cross-validation.

Note: We've shortened the training to 1 epoch for reasonable runtime.
Results indicate 15 epochs should be good.
"""

from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
def onehotencode_func(y):
  integer_encoded = y.reshape(len(y), 1)
  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
  return onehot_encoded

from sklearn.model_selection import StratifiedKFold

# 3-fold cross validation will be used because its computationally easier/faster
kfold = StratifiedKFold(n_splits=3, shuffle=True)

# the 'model_func' parameter is a lambda function
# 'lr' is a list with the values that we want to test
def test_params(lr, model_func):
  
  # dictionary where data will be stored and counter
  dic = {}
  i = 0.0

  # splitting data into the folds
  folds = kfold.split(x_train_short, y_train_short)
  for train_index, val_index in folds:

    # getting the model with the desired parameters
    model = model_func(lr)

    x_train_kf, x_val_kf =  x_train_short[train_index], x_train_short[val_index]
    y_train_kf, y_val_kf = onehotencode_func(y_train[train_index]), onehotencode_func(y_train[val_index])

    # training the model with data from the train data folds
    historytemp = model.fit(x_train_kf, y_train_kf, batch_size=32, epochs=1, validation_data=(x_val_kf, y_val_kf)) # 32, 15

    del model

    if dic == {}:
      # if dictionary is empty, values will be put there
      dic['train_acc'] = np.array(historytemp.history['accuracy'])
      dic['train_loss'] = np.array(historytemp.history['loss'])
      dic['val_acc'] = np.array(historytemp.history['val_accuracy'])
      dic['val_loss'] = np.array(historytemp.history['val_loss'])
    else:
      # if dictionary is not empty, values will be added element wise
      dic['train_acc'] += np.array(historytemp.history['accuracy'])
      dic['train_loss'] += np.array(historytemp.history['loss'])
      dic['val_acc'] += np.array(historytemp.history['val_accuracy'])
      dic['val_loss'] += np.array(historytemp.history['val_loss'])
    
    i+=1

  for k in dic:
    # each number in each array in the dictionary will be divided by the number of iterations, producing the mean of all the values read
    dic[k] /= i

  return dic

"""##Label Data

Used later to connect (model output) breed ID with its human-readable data.
"""

breed_names = [info_by_id[i]['breed'] for i in list(info_by_id.keys())]

"""#Model Training
Note: We've shortened the training to 1 epoch for reasonable runtime.

Results indicate 15 epochs should be good.
"""

model = getModel(learning_rate=0.001, dropout=0.35) # Create model
model.fit(x_train_short, onehotencode_func(y_train_short), batch_size=32, epochs=1) # Train model

"""#Fine-Tuning"""

# first we need to find the base model layer in our model
for l in model.layers:
    if l.name == 'inception_v3':
        base_model = l
        break

# then we unfreeze all of it
base_model.trainable = True

# fine-tune layers from 2/3 of the base model (that should be about 207)
fine_tune_at = 207

# freeze all the layers before the fine_tune_at layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable =  False

"""#Evaluate Model
Evaluate the model using the testing data.
"""

loss, acc = model.evaluate(x_test_short, onehotencode_func(y_test_short), verbose=2)
print("Loss: ", loss)
print("Acc: ", acc)

"""#User Interface
##Get Image
"""

import cv2
import urllib
import numpy as np
#img_url = "https://i.ytimg.com/vi/sXvSEVO1Heo/hqdefault.jpg"
req = urllib.request.urlopen(input("Enter Image URL: "))
arr = np.asarray(bytearray(req.read()), dtype=np.uint8)
img = cv2.imdecode(arr, -1) # 'Load it as it is'

# this is a trick to make the image be opened in RGB format, which is not the default
img_array = img[...,::-1] 
# here the images are rezise
img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))

predict_mat = model.predict(img_array.reshape(1, 299, 299, 3))
print("The given image shows a " + breed_names[predict_mat.argmax()] + " " + info_by_id[predict_mat.argmax()+1]['species'] + ".")

"""#Save Model
Save the model to a folder, zip it, then download it - disabled, used for debugging.
"""

from google.colab import files
model.save("my_model")
!zip -r /content/my_model.zip /content/my_model
files.download("/content/my_model.zip")